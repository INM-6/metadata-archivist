{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b476ac-a8eb-49fa-ad7d-c27256e45a2a",
   "metadata": {},
   "source": [
    "# Metadata Archivist\n",
    "\n",
    "**Author:** Jose Villamar\n",
    "\n",
    "The Metadata Archivist is a lightweight file processing framework designed to parse an arbitrary number of files with heterogenous formats and transform the results into a unified output.\n",
    "\n",
    "This tutorial is designed to guide users through the first steps needed to:\n",
    "1. Implement basic parsing functions by extending the abstract parsing class of the framework.\n",
    "2. Structure the output of the parsers by providing a schema template\n",
    "3. Instantiate and configure the Archivist controller to coordinate input files, parsers, and resulting output\n",
    "\n",
    "**Note:** Installation instructions can be found in our [README](../../README.md).\n",
    "\n",
    "## Example use case:\n",
    "\n",
    "![Illustration of the *Archivist*'s functionality in a simple example use case.](https://github.com/INM-6/metadata-archivist/raw/main/docs/Minimal_Example_Workflow_Schema.svg \"Illustration of the Archivist's functionality in a simple example use case\")\n",
    "\n",
    "In a parameter scanning experiment, several instances of a model with different configurations (parameters) are simulated (```Simulation 1 . . . Simulation N```; blue boxes on the left).\n",
    "During each simulation, configuration and performance information are recorded and stored in a (raw) metadata archive (yellow).\n",
    "After each simulation, the stored metadata is postprocessed (```Metadata post-processing 1 . . . Metadata post-processing N```; red):\n",
    "first, the relevant information is extracted by calling a predefined parsing script (gray box ```parsers.py```).\n",
    "Non-relevant information is discarded (see light gray text in the  raw metadata files).\n",
    "The extracted metadata are then structured according to a provided schema (gray box ```schema.json```).\n",
    "Finally, the simulation results are annotated with the structured metadata and stored in a database (red cylinder).\n",
    "After all simulations and the metadata post-processing are finished and their results stored in the database, the annotated data can be queried and presented (green).\n",
    "\n",
    "Excerpt from [DOI]()\n",
    "\n",
    "## Now we will implement the metadata post-processing pipeline step by step:\n",
    "\n",
    "### Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4f8fc5-878d-4e7b-93f4-ae851f8eccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the archivist framework.\n",
    "import metadata_archivist\n",
    "\n",
    "# Additional packages used in this tutorial.\n",
    "from json import dumps\n",
    "from pathlib import Path\n",
    "from yaml import safe_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3ba16-9299-498d-aa88-3878b1ff8bd0",
   "metadata": {},
   "source": [
    "### Input files\n",
    "\n",
    "As part of the tutorial an example collection of input files is provided in the ```raw_metadata``` directory.\n",
    "\n",
    "#### config.yml\n",
    "Generic configuration file containing several parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f30a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      "  sim_time: 100\n",
      "  scale: 10\n",
      "  num_procs: 2\n",
      "  threads_per_proc: 8\n",
      "  step_size: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Path(\"raw_metadata/config.yml\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d2d8d",
   "metadata": {},
   "source": [
    "#### time.txt\n",
    "Output of unix time wrapper command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40534b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m2.596s\n",
      "user\t0m15.543s\n",
      "sys\t0m0.632s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Path(\"raw_metadata/time.txt\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfbcc6-b446-4e74-a31f-c2b25a69a9db",
   "metadata": {},
   "source": [
    "### File parsers\n",
    "\n",
    "To be able to handle the input files parsers need to be defined, here we opt for one for each file.\n",
    "In principle, a parser could be implemented to process both files, however for simplicity and clarity we will proceed with two.\n",
    "\n",
    "**Note**: Although parsers need to be implemented to be able to interact with the framework, there may already exist processing frameworks for certain file types, in this case the parser implementation becomes a wrapper around these processing framework. In this example we will use [PyYAML](https://pypi.org/project/PyYAML/) to load the contents of ```config.yml```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ffb6a9-a966-47df-b417-2f14e7cabb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extend the abstract parser class from the metadata_archivist framework.\n",
    "class yml_parser(metadata_archivist.AParser):\n",
    "    # To instantiate the class a unique name is needed and the pattern of the file types that the parser can handle is needed.\n",
    "    # This pattern is defined as a regular expression pointing to the file name.\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            name=\"yml_parser\",\n",
    "            # Note that this parser will process any .yml file.\n",
    "            input_file_pattern=\".*\\.yml\",\n",
    "            # The output of the parser is described in the [JSON Schema](https://json-schema.org/overview/what-is-jsonschema) format.\n",
    "            schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sim_time\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"total time to simulate in seconds\",\n",
    "                            },\n",
    "                            \"scale\": {\"type\": \"number\", \"description\": \"model scale\"},\n",
    "                            \"num_procs\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"number of MPI processes\",\n",
    "                            },\n",
    "                            \"threads_per_proc\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"number of threads used per MPI process\",\n",
    "                            },\n",
    "                            \"step_size\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"step size for advancing simulation\",\n",
    "                            },\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Then the virtual parse method needs to be defined, here we use the yaml package to load the contents of the file.\n",
    "    def parse(self, file_path):\n",
    "        out = None\n",
    "        with file_path.open() as fp:\n",
    "            out = safe_load(fp)\n",
    "        return out\n",
    "\n",
    "\n",
    "# To help processing the time.txt file we create two functions:\n",
    "def time_parser_sec(string: str) -> float:\n",
    "    \"\"\" \"\n",
    "    This functions reads a string containing time duration information in ASCII format (output from the time command) and returns its value in seconds.\n",
    "\n",
    "    Arguments:\n",
    "        string: time duration information in ASCII format (output from the time command)\n",
    "\n",
    "    Returns:\n",
    "        float value of the duration in seconds\n",
    "    \"\"\"\n",
    "    minute_split = string.split(\"m\")\n",
    "    minutes = int(minute_split[0]) * 60 * 1000\n",
    "    second_split = minute_split[1].split(\".\")\n",
    "    seconds = int(second_split[0]) * 1000\n",
    "    milis = int(second_split[1][:-1])\n",
    "    return (minutes + seconds + milis) / 1000\n",
    "\n",
    "\n",
    "def key_val_split(string: str, split_char: str, functor=None):\n",
    "    \"\"\"\n",
    "    Basic ASCII text processing function.\n",
    "    Assumes input string contains a key value pair type of information, using a marker character splits the string and returns the key value pair as object.\n",
    "    Optionally values can be transformed using a provided function.\n",
    "\n",
    "    Arguments:\n",
    "        string: ASCII string containing key value pair information\n",
    "        split_char: ASCII character used as marker between key and value\n",
    "        functor: Optional. Callable used to transform value information\n",
    "\n",
    "    Returns:\n",
    "        object containing key value pair\n",
    "    \"\"\"\n",
    "    if functor is None:\n",
    "        functor = lambda x: x\n",
    "    string = string.strip()\n",
    "    out = string.split(split_char)\n",
    "    return {out[0].strip(): functor(out[1].strip())}\n",
    "\n",
    "\n",
    "# Now our second parser.\n",
    "class time_parser(metadata_archivist.AParser):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            name=\"time_parser\",\n",
    "            # Note that this parser will only process files named time.txt.\n",
    "            input_file_pattern=\"time\\.txt\",\n",
    "            # The output of the parser is described in the [JSON Schema](https://json-schema.org/overview/what-is-jsonschema) format.\n",
    "            schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"real\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"the time from start to finish of the call in seconds\",\n",
    "                    },\n",
    "                    \"user\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"amount of CPU time spent in user mode in seconds\",\n",
    "                    },\n",
    "                    \"sys\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"amount of CPU time spent in kernel mode in seconds\",\n",
    "                    },\n",
    "                    \"system\": {\"$ref\": \"#/properties/sys\"},\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def parse(self, file_path) -> dict:\n",
    "        out = {}\n",
    "        with file_path.open(\"r\") as fp:\n",
    "            for line in fp:\n",
    "                if line != \"\\n\":\n",
    "                    out.update(key_val_split(line, \"\\t\", time_parser_sec))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315634c9",
   "metadata": {},
   "source": [
    "### Structuring with a schema\n",
    "\n",
    "To be able to structure the output of parsers a schema template describing the combined output needs to be provided.\n",
    "This schema description is based on the [JSON Schema language](https://json-schema.org/learn/glossary) with additional clauses to describe structuring operations.\n",
    "JSON Schema is a declarative language used to define the structure and constraints of JSON data, enabling validation and ensuring data quality. It utilizes a set of keywords to specify the properties and rules for JSON documents, allowing developers to enforce consistency and interoperability across systems.\n",
    "\n",
    "\n",
    "Here we use:\n",
    "* ```!parsing``` directive which introduces the usage of parsing results of a single parser.\n",
    "* ```!calculate``` directive which combines through simple arithmetic the parsing results of two or more parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55647207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema template is defined as a dictionary object, naturally, it can also be stored as a JSON file.\n",
    "my_schema = {\n",
    "    # Dialect of JSONSchema known by the framework.\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    # As the template is based on the JSON Schema language, we use the standard headers for our schema files.\n",
    "    \"description\": \"my example schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        # Here property name refers to the actual keys in the output data.\n",
    "        \"real_time_factor\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"ratio of wall clock time to simulation time\",\n",
    "            # Calculate directive used to compute arithmetic expressions using parsing results of two or more parsers.\n",
    "            \"!calculate\": {\n",
    "                # The expression is defined as a string, variables can be defined inside of curly brackets.\n",
    "                \"expression\": \"{val1} / {val2}\",\n",
    "                # For each variable, its corresponding value from a parsing results needs to be defined.\n",
    "                \"variables\": {\n",
    "                    \"val1\": {\n",
    "                        # The parsing directive replaces the contents of the object with the parsing results of the referenced parser.\n",
    "                        \"!parsing\": {\n",
    "                            # Here we select a subset of the available results filtered using a specific key (or set of keys).\n",
    "                            \"keys\": [\"real\"],\n",
    "                            # The selection result is returned as a key value pair. If only the value is needed, it can be \"un-packed\".\n",
    "                            # Here we only need to unpack a single structure.\n",
    "                            \"unpack\": 1,\n",
    "                        },\n",
    "                        # Reference to the parser definition, in this framework parsers are automatically added to the definition sections of the schema.\n",
    "                        # These can be referenced through the unique name provided during class instantiation.\n",
    "                        \"$ref\": \"#/$defs/time_parser\",\n",
    "                    },\n",
    "                    \"val2\": {\n",
    "                        \"!parsing\": {\n",
    "                            # If the desired information is found inside a nested structure,\n",
    "                            # this nesting can be indicated as a path inside the structure where each key is separated by '/' .\n",
    "                            # Here subsequent keys means a deeper location inside the structure.\n",
    "                            \"keys\": [\"parameters/sim_time\"],\n",
    "                            # As we obtain a 2 level nested structure, the unpacking level also needs to be raised.\n",
    "                            \"unpack\": 2,\n",
    "                        },\n",
    "                        \"$ref\": \"#/$defs/yml_parser\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"!parsing\": {\n",
    "                \"keys\": [\"parameters/scale\"],\n",
    "                # Although a nested structure is selected here by parameters/scale (equivalent to parsing_results[\"parameters\"][\"scale\"]).\n",
    "                # We actually desire to have a scale: value key pair, hence we only use a single unpacking level.\n",
    "                \"unpack\": 1,\n",
    "            },\n",
    "            \"$ref\": \"#/$defs/yml_parser\",\n",
    "        },\n",
    "        \"virtual_processes\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"total number of digital processing units, number of processes multiplied by number of computing threads per process\",\n",
    "            \"!calculate\": {\n",
    "                \"expression\": \"{val1} * {val2}\",\n",
    "                \"variables\": {\n",
    "                    \"val1\": {\n",
    "                        \"!parsing\": {\n",
    "                            \"keys\": [\"parameters/num_procs\"],\n",
    "                            # In case we ultimately only desire terminal values from the selected structures,\n",
    "                            # unpack can be set to true and the selection result will be unpacked to the highest level possible.\n",
    "                            \"unpack\": True,\n",
    "                        },\n",
    "                        \"$ref\": \"#/$defs/yml_parser\",\n",
    "                    },\n",
    "                    \"val2\": {\n",
    "                        \"!parsing\": {\n",
    "                            \"keys\": [\"parameters/threads_per_proc\"],\n",
    "                            \"unpack\": True,\n",
    "                        },\n",
    "                        \"$ref\": \"#/$defs/yml_parser\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794d5ba",
   "metadata": {},
   "source": [
    "### Getting all together\n",
    "\n",
    "To be able to handle an arbitrary number of parsers and interpret the schema, the Metadata Archivist framework employs a ```Formatter``` class which handles parsing execution and structuring of its output.\n",
    "Together with the ```Archivist``` class which handles directory or archive exploration and exporting the structured output, the framework is capable of handling an arbitrary number of files and formats resulting in an unified structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c4cbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting schema:\n",
      "{\n",
      "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
      "    \"description\": \"my example schema\",\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "        \"real_time_factor\": {\n",
      "            \"type\": \"number\",\n",
      "            \"description\": \"ratio of wall clock time to simulation time\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"$ref\": \"#/$defs/yml_parser\"\n",
      "        },\n",
      "        \"virtual_processes\": {\n",
      "            \"type\": \"number\",\n",
      "            \"description\": \"total number of digital processing units, number of processes multiplied by number of computing threads per process\"\n",
      "        }\n",
      "    },\n",
      "    \"$defs\": {\n",
      "        \"node\": {\n",
      "            \"properties\": {\n",
      "                \"anyOf\": [\n",
      "                    {\n",
      "                        \"$ref\": \"#/$defs/time_parser\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"$ref\": \"#/$defs/yml_parser\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"$ref\": \"#/$defs/time_parser\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"$ref\": \"#/$defs/yml_parser\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"$ref\": \"#/$defs/time_parser\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"$ref\": \"#/$defs/yml_parser\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"time_parser\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"real\": {\n",
      "                    \"type\": \"number\",\n",
      "                    \"description\": \"the time from start to finish of the call in seconds\"\n",
      "                },\n",
      "                \"user\": {\n",
      "                    \"type\": \"number\",\n",
      "                    \"description\": \"amount of CPU time spent in user mode in seconds\"\n",
      "                },\n",
      "                \"sys\": {\n",
      "                    \"type\": \"number\",\n",
      "                    \"description\": \"amount of CPU time spent in kernel mode in seconds\"\n",
      "                },\n",
      "                \"system\": {\n",
      "                    \"$ref\": \"#/properties/sys\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"yml_parser\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"parameters\": {\n",
      "                    \"type\": \"object\",\n",
      "                    \"properties\": {\n",
      "                        \"sim_time\": {\n",
      "                            \"type\": \"number\",\n",
      "                            \"description\": \"total time to simulate in seconds\"\n",
      "                        },\n",
      "                        \"scale\": {\n",
      "                            \"type\": \"number\",\n",
      "                            \"description\": \"model scale\"\n",
      "                        },\n",
      "                        \"num_procs\": {\n",
      "                            \"type\": \"number\",\n",
      "                            \"description\": \"number of MPI processes\"\n",
      "                        },\n",
      "                        \"threads_per_proc\": {\n",
      "                            \"type\": \"number\",\n",
      "                            \"description\": \"number of threads used per MPI process\"\n",
      "                        },\n",
      "                        \"step_size\": {\n",
      "                            \"type\": \"number\",\n",
      "                            \"description\": \"step size for advancing simulation\"\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "Resulting metadata:\n",
      "{\n",
      "    \"real_time_factor\": {\n",
      "        \"value\": 0.02596,\n",
      "        \"description\": \"ratio of wall clock time to simulation time\",\n",
      "        \"type\": \"number\"\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"scale\": {\n",
      "            \"value\": 10,\n",
      "            \"description\": \"model scale\",\n",
      "            \"type\": \"number\"\n",
      "        }\n",
      "    },\n",
      "    \"virtual_processes\": {\n",
      "        \"value\": 16,\n",
      "        \"description\": \"total number of digital processing units, number of processes multiplied by number of computing threads per process\",\n",
      "        \"type\": \"number\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To instantiate the Archivist class:\n",
    "# a path to the desired exploration directory or archive,\n",
    "# a list of parsers to use,\n",
    "# and optionally a schema must be provided.\n",
    "arch = metadata_archivist.Archivist(\n",
    "    path=\"raw_metadata\",\n",
    "    parsers=[time_parser(), yml_parser()],\n",
    "    schema=my_schema,\n",
    "    add_description=True,\n",
    "    add_type=True,\n",
    "    verbosity=\"warning\",\n",
    ")\n",
    "\n",
    "arch.parse()\n",
    "\n",
    "print(\"Resulting schema:\")\n",
    "print(dumps(arch.get_formatted_schema(), indent=4))\n",
    "\n",
    "print(\"\\nResulting metadata:\")\n",
    "print(dumps(arch.get_metadata(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6aec6d",
   "metadata": {},
   "source": [
    "## Moving forward\n",
    "\n",
    "The newly structured metadata gives a quick description of the experiment configuration and results at hand and can be used to enrich the experiment data for further uses.\n",
    "\n",
    "As illustrated in the example use case shown at the beginning of this tutorial, the structured metadata can be fed to a database, then after several trials of simulation and reusing the post-processing pipeline, the database can be queried to extract broader observations on the simulation technology or model implementation.\n",
    "\n",
    "## Other examples\n",
    "\n",
    "This example builds upon all current features present in the framework, however a sequence examples showcasing each feature of the framework is available in the [examples directory](../../examples/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
